{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69674"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = 'reports_df.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "finding = \"Findings:\\n\\nLeft Ventricle: Normal size based on linear index dimension. Mildly decreased systolic function. Ejection fraction = 45 %. No regional wall motion abnormalities. Normal relative wall thickness and LV mass. Mild diastolic dysfunction. Normal filling pressures. No LV thrombus is seen.\\nRight Ventricle: Normal size by linear dimension. Normal systolic function. RV wall thickness is normal.\\nLeft Atrium: Moderately dilated by biplane volume index.\\nRight Atrium: Normal size by volume index.\\nMitral Valve: Normal valve leaflets. Mild annular calcification. Trivial regurgitation.\\nTricuspid Valve: Normal valve leaflets. Normal mobility of the leaflets. Trivial regurgitation.\\nAortic Valve: Trileaflet valve; sclerosis. Trivial valvular regurgitation.\\nPulmonary Valve/Artery: Not well seen. Mild regurgitation. Normal pulmonary artery systolic pressure: 24 mmHg.\\nAorta: Normal sinuses of Valsalva by index dimension. Normal sinuses of Valsalva by linear dimension. Normal proximal ascending aorta by index dimension. Normal proximal ascending aorta by linear dimension.\\nVenous: Inferior vena cava is normal size with greater than 50% respiratory variation.\\nPericardium/Other: Trivial pericardial effusion.\\nShunts: Atrial septum bulges rightward. No shunt by color flow Doppler to suggest PFO/ASD.\\n \\n\"\n",
    "finding = df['findings_txt'][0]\n",
    "finding = \"Findings:\\n\\nLeft Ventricle: Normal size based on linear index dimension. Mildly decreased systolic function. Ejection fraction = 45 %. No regional wall motion abnormalities. Normal relative wall thickness and LV mass. Mild diastolic dysfunction. Normal filling pressures. No LV thrombus is seen.\\nRight Ventricle: Normal size by linear dimension. Normal systolic function. RV wall thickness is normal.\\nLeft Atrium: Moderately dilated by biplane volume index.\\nRight Atrium: Normal size by volume index.\\nMitral Valve: Normal valve leaflets. Mild annular calcification. Trivial regurgitation.\\nTricuspid Valve: Normal valve leaflets. Normal mobility of the leaflets. Trivial regurgitation.\\nAortic Valve: Trileaflet valve; sclerosis. Trivial valvular regurgitation.\\nPulmonary Valve/Artery: Not well seen. Mild regurgitation. Normal pulmonary artery systolic pressure: 24 mmHg.\\nAorta: Normal sinuses of Valsalva by index dimension. Normal sinuses of Valsalva by linear dimension. Normal proximal ascending aorta by index dimension. Normal proximal ascending aorta by linear dimension.\\nVenous: Inferior vena cava is normal size with greater than 50% respiratory variation.\\nPericardium/Other: Trivial pericardial effusion.\\nShunts: Atrial septum bulges rightward. No shunt by color flow Doppler to suggest PFO/ASD.\\n \\n\"\n",
    "unique_sections = []\n",
    "unique_findings = {}\n",
    "\n",
    "# Remove the initial 'Findings:' part\n",
    "finding = finding.replace('Findings:', '').strip()\n",
    "\n",
    "# Split the string into sections\n",
    "sections = re.split(r'\\n(?=[A-Z])', finding)\n",
    "\n",
    "for section in sections:\n",
    "    if ':' in section:  # only process sections that contain findings\n",
    "        # Extract section name and findings\n",
    "        section_name, section_findings = section.split(':', 1)\n",
    "        section_name = section_name.strip()\n",
    "\n",
    "        # Add section name to unique_sections\n",
    "        unique_sections.append(section_name)\n",
    "\n",
    "        # Clean up section_findings and split them into individual findings by period\n",
    "        section_findings = [f.strip() for f in section_findings.split('.') if f.strip() != '']\n",
    "\n",
    "        # Remove any numeric data\n",
    "        section_findings = [re.sub(r'\\d+.*', '', f) for f in section_findings]\n",
    "\n",
    "        # If the section does not exist yet in the dictionary, add it\n",
    "        if section_name not in unique_findings:\n",
    "            unique_findings[section_name] = set()\n",
    "\n",
    "        # Add section_findings to unique_findings\n",
    "        unique_findings[section_name].update(section_findings)\n",
    "\n",
    "print(\"Unique sections:\")\n",
    "print(unique_sections)\n",
    "\n",
    "print(\"\\nUnique findings:\")\n",
    "for section_name, findings in unique_findings.items():\n",
    "    print(section_name)\n",
    "    for finding in findings:\n",
    "        print(finding)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "findings_text = df['findings_txt']\n",
    "# findings_text = findings_text[:10]\n",
    "section_frequencies = defaultdict(int)\n",
    "unique_findings = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for finding_text in findings_text:\n",
    "    # Remove the initial 'Findings:' part\n",
    "    finding_text = finding_text.replace('Findings:', '').strip()\n",
    "\n",
    "    # Replace double backslashes with actual newline characters\n",
    "    finding_text = finding_text.replace('\\\\n', '\\n')\n",
    "\n",
    "    # Split the string into sections\n",
    "    sections = re.split(r'\\n(?=[A-Z])', finding_text)\n",
    "    for section in sections:\n",
    "        if ':' in section:  # only process sections that contain findings\n",
    "            # Extract section name and findings\n",
    "            section_name, section_findings = section.split(':', 1)\n",
    "            section_name = section_name.strip()\n",
    "\n",
    "            # Increment the count for this section name\n",
    "            section_frequencies[section_name] += 1\n",
    "\n",
    "            # Clean up section_findings and split them into individual findings by period\n",
    "            section_findings = [f.strip() for f in section_findings.split('.') if f.strip() != '']\n",
    "\n",
    "            # Remove any numeric data\n",
    "            section_findings = [re.sub(r'\\d+.*', '', f) for f in section_findings]\n",
    "\n",
    "            # Increment the count for each finding within this section\n",
    "            for finding in section_findings:\n",
    "                unique_findings[section_name][finding] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Section frequencies:\")\n",
    "for section, count in section_frequencies.items():\n",
    "    print(f\"{section}: {count}\")\n",
    "\n",
    "print(\"\\nUnique findings:\")\n",
    "for section_name, findings in unique_findings.items():\n",
    "    print(section_name)\n",
    "    for finding, count in findings.items():\n",
    "        print(f\"{finding}: {count}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Section frequencies (by descending count):\")\n",
    "for section, count in sorted(section_frequencies.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{section}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnique findings (by descending count): \\n\")\n",
    "for section_name, findings in unique_findings.items():\n",
    "    print(section_name + \"\\n\")\n",
    "\n",
    "    sorted_findings = sorted(findings.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_50_findings = sorted_findings[:50]\n",
    "    total_count = sum(findings.values())\n",
    "    top_50_count = sum(count for finding, count in top_50_findings)\n",
    "\n",
    "    for finding, count in top_50_findings:\n",
    "        print(f\"{finding}: {count}\")\n",
    "\n",
    "    percentage = (top_50_count / total_count) * 100 if total_count > 0 else 0\n",
    "    print(f\"\\nThe top 50 findings make up {percentage:.2f}% of the total.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sections\n",
    "total_section_counts = sum(section_frequencies.values())\n",
    "over_1000_sections = {section: count for section, count in section_frequencies.items() if count > 1000}\n",
    "over_1000_section_counts = sum(over_1000_sections.values())\n",
    "percentage_over_1000_section_counts = (over_1000_section_counts / total_section_counts) * 100\n",
    "percentage_over_1000_sections_keys = (len(over_1000_sections) / len(section_frequencies)) * 100\n",
    "\n",
    "print(f\"Percentage of total section count taken by sections with count over 1000: {percentage_over_1000_section_counts}%\")\n",
    "print(f\"Percentage of section keys with count over 1000: {percentage_over_1000_sections_keys}%\")\n",
    "\n",
    "# For findings\n",
    "total_finding_counts = sum(count for findings in unique_findings.values() for count in findings.values())\n",
    "over_1000_findings = {section: {finding: count for finding, count in findings.items() if count > 1000} for section, findings in unique_findings.items()}\n",
    "over_1000_finding_counts = sum(count for findings in over_1000_findings.values() for count in findings.values())\n",
    "percentage_over_1000_finding_counts = (over_1000_finding_counts / total_finding_counts) * 100\n",
    "percentage_over_1000_findings_keys = (sum(len(findings) for findings in over_1000_findings.values()) / sum(len(findings) for findings in unique_findings.values())) * 100\n",
    "\n",
    "print(f\"Percentage of total finding count taken by findings with count over 1000: {percentage_over_1000_finding_counts}%\")\n",
    "print(f\"Percentage of finding keys with count over 1000: {percentage_over_1000_findings_keys}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Section frequencies (alphabetically):\")\n",
    "for section, count in sorted(section_frequencies.items(), key=lambda item: item[0]):\n",
    "    print(f\"{section}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnique findings (by descending count):\")\n",
    "for section_name, findings in unique_findings.items():\n",
    "    print(section_name)\n",
    "\n",
    "    sorted_findings = sorted(findings.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_50_findings = sorted_findings[:100]\n",
    "    total_count = sum(findings.values())\n",
    "    top_50_count = sum(count for finding, count in top_50_findings)\n",
    "\n",
    "    for finding, count in top_50_findings:\n",
    "        print(f\"{finding}: {count}\")\n",
    "\n",
    "    percentage = (top_50_count / total_count) * 100 if total_count > 0 else 0\n",
    "    print(f\"\\nThe top 100 findings make up {percentage:.2f}% of the total.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "measurement_counts = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['calcs_txt'].replace('\\\\n', '  ')\n",
    "    matches = re.findall(r'  (.*?)(:|\\')', text)\n",
    "    \n",
    "    for match, boundary in matches:\n",
    "        # Cut off any text up until a closing parenthesis \")\" if it exists\n",
    "        closing_parenthesis_index = match.rfind(')')\n",
    "        if closing_parenthesis_index != -1:\n",
    "            match = match[closing_parenthesis_index+1:]\n",
    "        \n",
    "        last_space_index = match.rfind('  ')\n",
    "        if last_space_index != -1:\n",
    "            match = match[last_space_index+2:]\n",
    "        \n",
    "        # Remove trailing or leading spaces and the colon\n",
    "        measurement_name = match.strip().rstrip(':')\n",
    "        \n",
    "        if measurement_name in measurement_counts:\n",
    "            measurement_counts[measurement_name] += 1\n",
    "        else:\n",
    "            measurement_counts[measurement_name] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Atrium 136822\n",
      " 105865\n",
      "Left Ventricle 69568\n",
      "LVIDd 69331\n",
      "LVIDd index 69252\n",
      "LVPWd 69250\n",
      "Right Ventricle 69248\n",
      "IVSd 69247\n",
      "LV RWT 69245\n",
      "Aorta 69232\n",
      "LV Mass index 69068\n",
      "LVIDs 69014\n",
      "LV Diastolic Function 68739\n",
      "RA Volume index 68709\n",
      "Aorta Sinuses 68639\n",
      "Aorta Sinuses index 68556\n",
      "RVd A4C 67780\n",
      "Tricuspid Valve and PA/RV Systolic Pressure 67262\n",
      "MV Peak E 66895\n",
      "LVOT diam 65903\n",
      "Septal e 65744\n",
      "Lateral e 65722\n",
      "Average E/e 65021\n",
      "Prox Asc Aorta index 64814\n",
      "Decel Time 64469\n",
      "TAPSE 63667\n",
      "MV Peak A 57318\n",
      "MV E/A ratio 57015\n",
      "Prox Ascending Aorta 53628\n",
      "RV S 37923\n",
      "LA Volume index 29018\n",
      "Aortic Valve 25233\n",
      "LV EDV index 23533\n",
      "LV ESV index 23412\n",
      "AoV Vmax 18002\n",
      "AoV Peak PG 17936\n",
      "SV LVOT 17185\n",
      "Mitral Valve 14245\n",
      "Prox Asc Aorta 11222\n",
      "CO LVOT 11163\n",
      "Cardiac Index 11126\n",
      "TV S 10774\n",
      "AR P1/2T 9885\n",
      "Pulmonary Regurgitation 8641\n",
      "PR EDV 8641\n",
      "MV Mean Grad 8585\n",
      "AoV Mean PG 7507\n",
      "Aov Mean PG 7483\n",
      "MR EROA 6498\n",
      "Right Atrium 5017\n",
      "Right Atrium index 5010\n",
      "MR Volume 4938\n",
      "PLAX RVOT Diam 2904\n",
      "AR EROA 2070\n",
      "Prox RVOT Diam 2051\n",
      "AR Volume 1737\n",
      "Pulmonary Valve 1596\n",
      "LVIDs index 1178\n",
      "PV Max Velocity 1165\n",
      "AVA VTI index 1060\n",
      "GLS Strain 1029\n",
      "2.2 m/s AVA, VTI 913\n",
      "TV Mean PG 882\n",
      "2.1 m/s AVA, VTI 827\n",
      "2.0 m/s AVA, VTI 793\n",
      "SV LVOT index 792\n",
      "2.3 m/s AVA, VTI 786\n",
      "PAT 772\n",
      "MR Vena Contracta 737\n",
      "AVA, VTI index 689\n",
      "2.4 m/s AVA, VTI 687\n",
      "AR Vena Contracta 672\n",
      "Aortic Arch 666\n",
      "2.5 m/s AVA, VTI 641\n",
      "Pulmonary Arteries 636\n",
      "PV Max PG 611\n",
      "1.9 m/s AVA, VTI 605\n",
      "2.6 m/s AVA, VTI 605\n",
      "1.8 m/s AVA, VTI 543\n",
      "Distal RVOT Diam 511\n",
      "2.8 m/s AVA, VTI 495\n",
      "2.7 m/s AVA, VTI 472\n",
      "1.7 m/s AVA, VTI 430\n",
      "RVSP 423\n",
      "1.6 m/s AVA, VTI 408\n",
      "1.5 m/s AVA, VTI 373\n",
      "2.9 m/s AVA, VTI 370\n",
      "RVOT Vmax 360\n",
      "3.0 m/s AVA, VTI 341\n",
      "1.4 m/s AVA, VTI 321\n",
      "3.1 m/s AVA, VTI 319\n",
      "Descending Aorta 312\n",
      "1.3 m/s AVA, VTI 309\n",
      "LVOT Diam 306\n",
      "PR P1/2 Time 300\n",
      "3.2 m/s AVA, VTI 298\n",
      "1.2 m/s AVA, VTI 296\n",
      "MVA, P1/2 Time 286\n",
      "3.3 m/s AVA, VTI 279\n",
      "3.4 m/s AVA, VTI 273\n"
     ]
    }
   ],
   "source": [
    "sorted_measurement_counts = {k.rstrip(':'): v for k, v in sorted(measurement_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Print the top 100 items\n",
    "top_100 = list(sorted_measurement_counts.items())[:100]\n",
    "for measurement, count in top_100:\n",
    "    print(measurement, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Left Ventricle:', 'Left Atrium:', 'IVSd:', 'Left Atrium:', 'LVPWd:', 'LA vol index (Biplane):', 'LVIDd:', 'Right Ventricle:', 'LVIDd index:', 'RVd A4C:', 'LVIDs:', \"RV S'\", 'LV EF (Biplane):', 'TAPSE:', 'LV Mass index:', 'LV RWT:', 'RA Volume index:', 'LV EDV index:', 'Aorta:', 'LV ESV index:', 'Aorta Sinuses:', 'LVOT diam:', 'Aorta Sinuses index:', 'LV Diastolic Function:', 'Prox Ascending Aorta:', 'Prox Asc Aorta index:', 'MV Peak E:', 'Decel Time:', \"Lateral e'\", \"Septal e'\", \"Average E/e'\", 'Tricuspid Valve and PA/RV Systolic Pressure:']\n"
     ]
    }
   ],
   "source": [
    "text = df['calcs_txt'][0].replace('\\\\n', '  ')\n",
    "\n",
    "matches = re.findall(r'  (.*?)(:|\\')', text)\n",
    "\n",
    "modified_matches = []\n",
    "\n",
    "for match, boundary in matches:\n",
    "    last_space_index = match.rfind('  ')\n",
    "    if last_space_index != -1:\n",
    "        modified_match = match[last_space_index+2:]\n",
    "    else:\n",
    "        modified_match = match\n",
    "    modified_matches.append(modified_match + boundary)\n",
    "\n",
    "print(modified_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements and Calculations:\n",
      "Left Ventricle:             Normal   Left Atrium:                      Normal\n",
      "IVSd:            10 mm      (6-10)   Left Atrium:            39 mm     (30-40)\n",
      "LVPWd:           10 mm      (6-10)   LA vol index (Biplane): *44 ml/m\\xb2 (16-34)\n",
      "LVIDd:           51 mm      (42-58)  Right Ventricle:            Normal\n",
      "LVIDd index:     24 mm/m\\xb2   (22-30)  RVd A4C:         34 mm      (25-41)\n",
      "LVIDs:           39 mm      (25-40)  RV S'            10.5 cm/s  (=10)\n",
      "LV EF (Biplane): 45 %       (52-72)  TAPSE:           20 mm      (>17)\n",
      "LV Mass index:   88.9 g/m\\xb2  (49-115) Right Atrium              Normal\n",
      "LV RWT:          0.38       (<0.43)  RA Volume index: 23 ml/m\\xb2 (18-39)\n",
      "LV EDV index:    57.6 ml/m\\xb2 (34-74)  Aorta:                         Normal\n",
      "LV ESV index:    31.4 ml/m\\xb2 (11-31)  Aorta Sinuses:        40 mm    (=40)\n",
      "LVOT diam:       21 mm               Aorta Sinuses index:  19 mm/m\\xb2 (=21)\n",
      "LV Diastolic Function:               Prox Ascending Aorta: 32 mm    (=38)\n",
      "                                     Prox Asc Aorta index: 15 mm/m\\xb2 (=19)\n",
      "MV Peak E:  59.9 cm/s\n",
      "Decel Time: 167 msec\n",
      "Lateral e':         11.9 cm/s\n",
      "Septal e':          7.4 cm/s\n",
      "Average E/e' Ratio: 6.2\n",
      "\n",
      " \n",
      "Tricuspid Valve and PA/RV Systolic Pressure: TR Max Velocity: 2.3 m/s RA Pressure: 3 mmHg PASP: 24 mmHg\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['calcs_txt'][0].replace('\\\\n','\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "manual_string = raw_text\n",
    "dataframe_value = df['calcs_txt'][0]\n",
    "\n",
    "# Check the lengths of the strings\n",
    "if len(manual_string) != len(dataframe_value):\n",
    "    print(\"Lengths of the strings are different.\")\n",
    "\n",
    "# Compare the hexadecimal representations of the characters\n",
    "for i, (ch1, ch2) in enumerate(zip(manual_string, dataframe_value)):\n",
    "    if ch1.encode().hex() != ch2.encode().hex():\n",
    "        print(f\"Difference found at index {i}: '{ch1}' (manual string) vs '{ch2}' (DataFrame value)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the boolean mask to filter the DataFrame\n",
    "date_df = sample_df[mask]\n",
    "\n",
    "# Extract the years from the 'full_text' column\n",
    "years = date_df['full_text'].str.extract(date_pattern)[0].str[-4:]\n",
    "\n",
    "# Convert the extracted years to integers\n",
    "years = years.astype(int)\n",
    "\n",
    "# Plot a histogram of the years\n",
    "plt.hist(years, bins=range(years.min(), years.max()+1), edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(years[years >= 2020].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the filtered DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "first_row = df.iloc[0].to_dict()\n",
    "\n",
    "for col, value in first_row.items():\n",
    "    if isinstance(value, str):  # check if the value is a string\n",
    "        value = value.encode('utf-8').decode('unicode_escape')  # interpret the special characters\n",
    "    print(f\"{col}:\\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_measurements(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    data = {}\n",
    "    for line in lines:\n",
    "        parts = [part.strip() for part in line.split(':', 1)]\n",
    "        if len(parts) == 2:\n",
    "            key, value = parts\n",
    "            value_parts = value.rsplit('(', 1)\n",
    "            if len(value_parts) == 2:\n",
    "                value_parts[1] = value_parts[1].rstrip(')')\n",
    "                data[key] = value_parts[0].strip()\n",
    "                data[key + ' normal range'] = value_parts[1]\n",
    "            else:\n",
    "                data[key] = value_parts[0]\n",
    "    return data\n",
    "\n",
    "parsed_data = df['calcs_txt'].apply(parse_measurements)\n",
    "parsed_df = pd.DataFrame(list(parsed_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8777566cbcc6fa3a8138ef1599f67d7a44f1cf90229c5fbcba43102b1bb48f50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
